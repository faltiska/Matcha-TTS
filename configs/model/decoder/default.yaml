#  Decoder capacity increase:
#  channels 256 → 384
#  attention_head_dim 64 → 80
#  n_blocks: 1 → 3
#  Decoder model size increased from 15 M to 24 M params 
channels: [384, 384]
dropout: 0.05
attention_head_dim: 80
n_blocks: 3
num_mid_blocks: 2
num_heads: 2

# Snake β activation (implemented in matcha/models/components/transformer.py)
act_fn: snakebeta 
